{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8RZOuS9LWQvv"},"outputs":[],"source":["# import libraries\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  !pip install tf-nightly\n","except Exception:\n","  pass\n","import tensorflow as tf\n","import pandas as pd\n","from tensorflow import keras\n","!pip install tensorflow-datasets\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMHwYXHXCar3"},"outputs":[],"source":["# get data files\n","!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n","!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n","\n","train_file_path = \"train-data.tsv\"\n","test_file_path = \"valid-data.tsv\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_h508FEClxO"},"outputs":[],"source":["df = pd.read_csv(\"train-data.tsv\",sep='\\t', header= None)\n","df.columns =['label', 'message']\n","df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOMKywn4zReN"},"outputs":[],"source":["df.describe()"]},{"cell_type":"code","source":["df.groupby('label').describe().T"],"metadata":{"id":"RX9CF1tDN_hp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get all the ham and spam emails\n","ham_msg = df[df.label =='ham']\n","spam_msg = df[df.label=='spam']# Create numpy list to visualize using wordcloud\n","ham_msg_text = \" \".join(ham_msg.message.to_numpy().tolist())\n","spam_msg_text = \" \".join(spam_msg.message.to_numpy().tolist())"],"metadata":{"id":"w4Sg2lNSN_TV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","ham_msg_cloud = WordCloud(width =320, height =160, stopwords=STOPWORDS,max_font_size=50, background_color =\"black\", colormap='Blues').generate(ham_msg_text)\n","plt.figure(figsize=(10,8))\n","plt.imshow(ham_msg_cloud, interpolation='bilinear')\n","plt.axis('off') # turn off axis\n","plt.show()"],"metadata":{"id":"usFBw1BwN_H3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","ham_msg_cloud = WordCloud(width =320, height =160, stopwords=STOPWORDS,max_font_size=50, background_color =\"white\", colormap='autumn').generate(spam_msg_text)\n","plt.figure(figsize=(10,8))\n","plt.imshow(ham_msg_cloud, interpolation='bilinear')\n","plt.axis('off') # turn off axis\n","plt.show()"],"metadata":{"id":"D7rksY7UN-_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","plt.figure(figsize=(8,6))\n","sns.countplot(df.label)\n","# Percentage of spam messages\n","(len(spam_msg)/len(ham_msg))*100 # 15.48%"],"metadata":{"id":"Egf-978rN-0q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# one way to fix it is to downsample the ham msg\n","ham_msg_df = ham_msg.sample(n = len(spam_msg), random_state = 0)\n","spam_msg_df = spam_msg\n","print(ham_msg_df.shape, spam_msg_df.shape)#(747, 2) (747, 2)"],"metadata":{"id":"zdeKGsgIN-ps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()\n","df.groupby('label').describe().T\n","\n","# Get all the ham and spam messages\n","ham_msg = df[df.label == 'ham']\n","spam_msg = df[df.label == 'spam']\n","\n","# Create numpy list to visualize using wordcloud\n","ham_msg_text = \" \".join(ham_msg.message.to_numpy().tolist())\n","spam_msg_text = \" \".join(spam_msg.message.to_numpy().tolist())\n","\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","\n","# Plot word cloud for ham messages\n","ham_msg_cloud = WordCloud(width=320, height=160, stopwords=STOPWORDS, max_font_size=50, background_color=\"black\", colormap='Blues').generate(ham_msg_text)\n","plt.figure(figsize=(10, 8))\n","plt.imshow(ham_msg_cloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.show()\n","\n","# Plot word cloud for spam messages\n","spam_msg_cloud = WordCloud(width=320, height=160, stopwords=STOPWORDS, max_font_size=50, background_color=\"white\", colormap='autumn').generate(spam_msg_text)\n","plt.figure(figsize=(10, 8))\n","plt.imshow(spam_msg_cloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.show()\n","\n","import seaborn as sns\n","plt.figure(figsize=(8, 6))\n","sns.countplot(df.label)\n","plt.title('Distribution of Ham and Spam Messages')\n","plt.show()\n","\n","# Percentage of spam messages\n","(len(spam_msg) / len(ham_msg)) * 100 # Approximately 15.48%\n"],"metadata":{"id":"5XcBjjegN-bF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Downsample ham messages\n","ham_msg_df = ham_msg.sample(n=len(spam_msg), random_state=0)\n","spam_msg_df = spam_msg\n","msg_df = pd.concat([ham_msg_df, spam_msg_df]).reset_index(drop=True)\n","\n","plt.figure(figsize=(8, 6))\n","sns.countplot(msg_df.label)\n","plt.title('Distribution of Ham and Spam Messages (After Downsampling)')\n","plt.show()\n","\n","# Add length column\n","msg_df['text_length'] = msg_df['message'].apply(len)\n","msg_df['msg_type'] = msg_df['label'].map({'ham': 0, 'spam': 1})\n","\n","df_test = pd.read_csv(test_file_path, sep='\\t', header=None)\n","df_test.columns = ['label', 'message']\n","df_test['msg_type'] = df_test['label'].map({'ham': 0, 'spam': 1})\n","\n","train_label = msg_df['msg_type']\n","train_msg = msg_df['message']\n","test_msg = df_test['message']\n","test_label = df_test['msg_type']\n"],"metadata":{"id":"8_QYoGshPC_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining pre-processing hyperparameters\n","max_len = 50\n","trunc_type = \"post\"\n","padding_type = \"post\"\n","oov_tok = \"\"\n","vocab_size = 500\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer(num_words=vocab_size, char_level=False, oov_token=oov_tok)\n","tokenizer.fit_on_texts(train_msg)\n","\n","word_index = tokenizer.word_index\n","tot_words = len(word_index)\n","print('There are %s unique tokens in training data.' % tot_words)\n","\n","training_sequences = tokenizer.texts_to_sequences(train_msg)\n","training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n","testing_sequences = tokenizer.texts_to_sequences(test_msg)\n","testing_padded = pad_sequences(testing_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n","\n","print('Shape of training tensor: ', training_padded.shape)\n","print('Shape of testing tensor: ', testing_padded.shape)\n"],"metadata":{"id":"gEsItHq2Pr6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = 500\n","embedding_dim = 16\n","dropout_rate = 0.2\n","n_dense = 24\n","\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n","model.add(GlobalAveragePooling1D())\n","model.add(Dense(n_dense, activation='relu'))\n","model.add(Dropout(dropout_rate))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.summary()\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","num_epochs = 30\n","early_stop = EarlyStopping(monitor='val_loss', patience=3)\n","history = model.fit(training_padded, train_label, epochs=num_epochs, validation_data=(testing_padded, test_label), callbacks=[early_stop], verbose=2)\n","\n","model.evaluate(testing_padded, test_label)\n","\n","metrics = pd.DataFrame(history.history)\n","metrics.rename(columns={'loss': 'Training_Loss', 'accuracy': 'Training_Accuracy', 'val_loss': 'Validation_Loss', 'val_accuracy': 'Validation_Accuracy'}, inplace=True)\n","\n","def plot_graphs1(var1, var2, string):\n","    metrics[[var1, var2]].plot()\n","    plt.title('Training and Validation ' + string)\n","    plt.xlabel('Number of epochs')\n","    plt.ylabel(string)\n","    plt.legend([var1, var2])\n","\n","plot_graphs1('Training_Accuracy', 'Validation_Accuracy', 'accuracy')\n"],"metadata":{"id":"XecSh32hPzmd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xCNXQ8P5P5Ik"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9tD9yACG6M9"},"outputs":[],"source":["# Function to predict messages based on model\n","def predict_message(pred_text1):\n","    pred_text = [pred_text1]\n","    new_seq = tokenizer.texts_to_sequences(pred_text)\n","    padded = pad_sequences(new_seq, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n","    prediction = model.predict(padded)\n","    if prediction[0] > 0.5:\n","        return [float(prediction[0]), \"spam\"]\n","    else:\n","        return [float(prediction[0]), \"ham\"]\n","\n","# Test prediction function\n","pred_text = \"you have won £1000 cash! call to claim\"\n","prediction = predict_message(pred_text)\n","print(prediction)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dxotov85SjsC"},"outputs":[],"source":["# Run this cell to test your function and model. Do not modify contents.\n","def test_predictions():\n","  test_messages = [\"how are you doing today\",\n","                   \"sale today! to stop texts call 98912460324\",\n","                   \"i dont want to go. can we try it a different day? available sat\",\n","                   \"our new mobile video service is live. just install on your phone to start watching.\",\n","                   \"you have won £1000 cash! call to claim your prize.\",\n","                   \"i'll bring it tomorrow. don't forget the milk.\",\n","                   \"wow, is your arm alright. that happened to me one time too\"\n","                  ]\n","\n","  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n","  passed = True\n","\n","  for msg, ans in zip(test_messages, test_answers):\n","    prediction = predict_message(msg)\n","    if prediction[1] != ans:\n","      passed = False\n","\n","  if passed:\n","    print(\"You passed the challenge. Great job!\")\n","  else:\n","    print(\"You haven't passed yet. Keep trying.\")\n","\n","test_predictions()\n"]}],"metadata":{"colab":{"name":"fcc_sms_text_classification.ipynb","private_outputs":true,"provenance":[{"file_id":"https://github.com/freeCodeCamp/boilerplate-neural-network-sms-text-classifier/blob/master/fcc_sms_text_classification.ipynb","timestamp":1724685226214}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{}},"nbformat":4,"nbformat_minor":0}